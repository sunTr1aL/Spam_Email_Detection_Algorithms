## CS410 Final Project - RoBERTa Part


### Spam E-mail Detection Based on RoBERTa
- Using the pre-trained RoBERTa model
- Include three datasets: Enron, Spam Assassin, and TREC 2007
- Training sets for parameter tuning and test sets for testing are generated using methods such as random partitioning, time sorting, cross-tabulation, and proportional partitioning.
- Calculating the acc, prec, recall, and F1 values, as well as drawing the confusion matrix


### Project file structure:

```
/data/processed_data/:    # Preprocessed dataset
--enron_clean.csv:          # Preprocessed Enron dataset
--spam_assassin_clean.csv:  # Preprocessed Spam_assassin dataset
--trec2007_clean.csv:       # Preprocessed Trec2007 dataset
/code/roberta/:           # Code
--config.py:                # Program runtime configuration
--dataset.py:               # Dataset splitting code
--evaluator.py:             # Roberta evaluation related code
--trainer.py:               # Roberta training related code
--run.py:                   # Program entry point
--summary.py:               # Summarize output and generate a final csv file
--run_log.txt:              # Program runtime output
/roberta_output/:         # Output
--random_split/:            # Random split results (containing 3 datasets)
--time_split/:              # Time split results (containing only Enron dataset)
--cross_dataset/:           # Cross-dataset training & validation (containing 3 datasets)
--learning_curve/:          # Results with different training set ratios (containing 3 datasets)
```


### Getting Started
1. Activate Python virtual environment;
2. Install Python packages `transformers`, `torch`, `sklearn`, `pandas`, `matplotlib`, `seaborn`;
3. Go to base directory `cd code/roberta/`;
4. Change configurations in `config.py` if needed;
5. Run `python3 run.py`;
6. Results are saved in **OUTPUT_DIR** folder in `config.py`.


### Running Details
The program uses pre-processed data **spam_assassin_clean.csv**, **enron_clean.csv**, **trec2007_clean.csv**
 generated by `baselines.py`. Make sure they are contained in the **DATA_DIR** folder in `config.py`.


### Results
| Dataset | Split | Model | Acc | Prec | Recall | F1 |
| ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| combined | weighted_assassin | RoBERTa | 0.9905 | 0.9868 | 0.9842 | 0.9855 |
| combined | weighted_cross_dataset | RoBERTa | 0.7891 | 0.7656 | 0.9224 | 0.8322 |
| combined | weighted_random | RoBERTa | 0.9951 | 0.9967 | 0.9943 | 0.9955 |
| combined | weighted_ratio_0.1 | RoBERTa | 0.9711 | 0.9387 | 0.9417 | 0.9402 |
| combined | weighted_ratio_0.2 | RoBERTa | 0.9874 | 0.9900 | 0.9832 | 0.9865 |
| combined | weighted_ratio_0.3 | RoBERTa | 0.9921 | 0.9934 | 0.9910 | 0.9922 |
| combined | weighted_ratio_0.4 | RoBERTa | 0.9930 | 0.9952 | 0.9913 | 0.9932 |
| combined | weighted_ratio_0.5 | RoBERTa | 0.9936 | 0.9956 | 0.9919 | 0.9938 |
| combined | weighted_ratio_0.6 | RoBERTa | 0.9942 | 0.9957 | 0.9930 | 0.9944 |
| combined | weighted_ratio_0.7 | RoBERTa | 0.9945 | 0.9958 | 0.9933 | 0.9946 |
| combined | weighted_time | RoBERTa | 0.9615 | 0.9977 | 0.9630 | 0.9800 |
| enron | random | RoBERTa | 0.9907 | 0.9941 | 0.9874 | 0.9908 |
| enron | ratio_0.1 | RoBERTa | 0.9785 | 0.9734 | 0.9843 | 0.9788 |
| enron | ratio_0.2 | RoBERTa | 0.9862 | 0.9855 | 0.9874 | 0.9864 |
| enron | ratio_0.3 | RoBERTa | 0.9874 | 0.9898 | 0.9851 | 0.9875 |
| enron | ratio_0.4 | RoBERTa | 0.9887 | 0.9933 | 0.9844 | 0.9888 |
| enron | ratio_0.5 | RoBERTa | 0.9902 | 0.9929 | 0.9877 | 0.9903 |
| enron | ratio_0.6 | RoBERTa | 0.9905 | 0.9943 | 0.9868 | 0.9905 |
| enron | ratio_0.7 | RoBERTa | 0.9921 | 0.9937 | 0.9906 | 0.9922 |
| enron | time | RoBERTa | 0.9615 | 0.9977 | 0.9630 | 0.9800 |
| enron_spam_assassin | cross_dataset | RoBERTa | 0.4142 | 0.3577 | 0.9958 | 0.5264 |
| enron_trec2007 | cross_dataset | RoBERTa | 0.8710 | 0.8533 | 0.9562 | 0.9018 |
| spam | assassin | RoBERTa | 0.9905 | 0.9868 | 0.9842 | 0.9855 |
| spam | ratio_0.1 | RoBERTa | 0.6731 | 0.0000 | 0.0000 | 0.0000 |
| spam | ratio_0.2 | RoBERTa | 0.9310 | 0.9295 | 0.8535 | 0.8899 |
| spam | ratio_0.3 | RoBERTa | 0.9786 | 0.9740 | 0.9600 | 0.9670 |
| spam | ratio_0.4 | RoBERTa | 0.9848 | 0.9738 | 0.9798 | 0.9768 |
| spam | ratio_0.5 | RoBERTa | 0.9838 | 0.9902 | 0.9599 | 0.9748 |
| spam | ratio_0.6 | RoBERTa | 0.9858 | 0.9789 | 0.9776 | 0.9782 |
| spam | ratio_0.7 | RoBERTa | 0.9845 | 0.9856 | 0.9665 | 0.9760 |
| spam_assassin_enron | cross_dataset | RoBERTa | 0.6243 | 0.5877 | 0.8652 | 0.6999 |
| spam_assassin_trec2007 | cross_dataset | RoBERTa | 0.7934 | 0.8001 | 0.8884 | 0.8419 |
| trec2007 | random | RoBERTa | 0.9969 | 0.9978 | 0.9972 | 0.9975 |
| trec2007 | ratio_0.1 | RoBERTa | 0.9895 | 0.9915 | 0.9915 | 0.9915 |
| trec2007 | ratio_0.2 | RoBERTa | 0.9920 | 0.9962 | 0.9908 | 0.9935 |
| trec2007 | ratio_0.3 | RoBERTa | 0.9951 | 0.9963 | 0.9957 | 0.9960 |
| trec2007 | ratio_0.4 | RoBERTa | 0.9954 | 0.9975 | 0.9950 | 0.9962 |
| trec2007 | ratio_0.5 | RoBERTa | 0.9957 | 0.9972 | 0.9960 | 0.9966 |
| trec2007 | ratio_0.6 | RoBERTa | 0.9964 | 0.9975 | 0.9968 | 0.9971 |
| trec2007 | ratio_0.7 | RoBERTa | 0.9962 | 0.9974 | 0.9964 | 0.9969 |
| trec2007_enron | cross_dataset | RoBERTa | 0.7987 | 0.7267 | 0.9658 | 0.8293 |
| trec2007_spam_assassin | cross_dataset | RoBERTa | 0.8661 | 0.7314 | 0.9329 | 0.8200 |